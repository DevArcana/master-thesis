\Chapter{Multi-agent systems and cognitive architectures}\label{chapter:chapter1}

\section{Agents and architecture}

Both the player and any non-playable characters can be called agents.
Each agent is independent from the rest and is able to act based on the subset of the plot available to them.
The actual implementation of each agent's behaviour can be as simple as a hardcoded line of dialogue and as complex as a set of rules enabling emergent behaviour.
General plot of the game as experienced by the player is the sum of interactions and sub-plots of the individual agents in the game.
Grey et al.\cite{grey2011procedural} proposed a solution using agent-based simulation to create procedural quests.
This work focuses on extension of the original approach using a SOAR cognitive architecture\cite{rosenbloom1993soar}.

\subsection{Cognitive architecture}

According to Langley et al.\cite{langley2009cognitive} a cognitive architecture specifies the underlying infrastructure for an intelligent system.
The authors describe the distinction between the agent's architecture and its memory and beliefs that can change over time, even proposing an analogy to a building with furniture in it, indicating that while the architecture of the building is constant, the elements within are not.
Among the architectures described by the authors are the following examples:

\begin{description}
    \item[ACT] Modular architecture focused on modelling human behaviour. Notable feature of this architecture is that each production rule is associated with a utility function that calculates the potential benefit from execution of the action.
    \item[SOAR] Architecture consisting of production rules and operators that influence the external or internal state of the agent.
    \item[ICARUS] ICARUS splits the memory into concepts, percepts and skills. The concepts and percepts describe the situation while skills allow for choosing an action to take. Skills represent goal concepts they aim to achieve. All memories are stored in a hierarchical way.
    \item[PRODIGY] This architecture defines the concept of domain rules that encode the conditions under which actions can be executed as well as control rules that define which actions are to be selected or rejected.
\end{description}

Another notable example in the space of cognitive architectures is belief-desire-intention (BDI) described by Georgeff in 1992\cite{georgeff1992abstract}.
In this architecture the agents hold certain beliefs about its state, has desire to act in a certain way and possesses intention that guarantees that it does not act randomly.

All cognitive architectures can be used to solve similar problems, albeit each with a different approach.
In this thesis the SOAR (State, Operator, And Representation) architecture was chosen \cite{laird2019soar}.
It allows the agents to have a set of rules that determine their behaviour as well as represent the state of the world as perceived by the agent in its working memory.
The simplicity and straightforwardness of SOAR is one of the main reasons for choosing this particular cognitive architecture.
Additionally, architectures that rely on utility functions that calculate the applicability of a production rule were excluded due to the fact that definition of utility for many aspects of simulation of a virtual character in game is adding extra complexity.
Another beneficial property of SOAR is the ability to express the production rules as independent and non-hierarchical.
In some games decision trees are enough to express the range of interactions with an agent\cite{sweetser2002current}.
The main limitation of this approach is that decision trees are hierarchical and no choice can be made in isolation as well as the fact that they are usually deterministic and thus predictable.
The latter limitation is avoidable as probabilistic decision trees can be used \cite{saks1986probabilistic}, however the former limitation is a charcteristic of all trees and relational graph structures.

SOAR is a cognitive architecture that is designed to simulate human thinking and problem-solving processes.
At its core, SOAR is a symbolic, production-based system that uses rules to guide behavior.
All production rules in this architecture are independent and concerned with a single specific behaviour or inference that the agent can perform.
The memory can store hierarchical information but the rules that govern the agent's behaviour are not hierarchical themselves.

This architecture consists of two major components:

\begin{itemize}
    \item Procedural memory (production memory) - set of production rules that take working memory elements and propose operators (behaviours) or modify the state of working memory (inferences)
    \item Working memory - fact based knowledge represented by a symbolic graph structure describing the current state of the world
\end{itemize}

Many implementations of this architecture additionally include the concept of long-term memory structure that accumulates knowledge during the whole lifetime of the agent.
In general, implementation of the SOAR architecture can be called a production system.
Unlike many traditional production systems however, all rules that match against the current state of the world will fire (execute actions) in parallel.
This means that the list of rules is not ordered and conflicting operators may be proposed.
In such cases the usual strategy is to create a substate of the working memory with a new goal of conflict resolution.

The SOAR architecture has been successfuly utilized in creation of video game AI agents.
Examples include the application of this architecture to create an autonomous agent designed for playing Quake\cite{laird2001knows}, StarCraft\cite{turner2013soar-sc} and Descent 3\cite{van1999developing}.
Michael van Lent and John Laird in their work "Developing an Artificial Intelligence Engine"\cite{van1999developing} describe a decision cycle in an inference machine:

\begin{itemize}
    \item Perceive: Accept sensor information from the game
    \item Think: Select and execute relevant knowledge
    \item Act: Execute actions in the game
\end{itemize}

This cycle combined with the SOAR architecture allows for creation of a versatile agent capable of processing and propagating plot events in the game.
The first step allows the agent to modify the state of the working memory.
It can detect visible objects, including other agents and interactables such as containers, doors or even writings.
The second step is divided into two substeps, the first executes inference rules and modfies the state of the working memory while the other part executes behaviour rules which propose operators.
Operators modify the state of the world, for example by moving the agent, taking the contents of a container, opening doors or interacting with another agent.
This constitutes the last step of the cycle and allows the agent to go back to the first step and update its working memory in preparation for the next step.

This architecture is versatile in that it can be used to model even very complex behaviours.
In order to avoid conflicts in the proposed operators by production rules that try to solve different goals, a goal token can be inserted into the working memory and used as a dependency in the production rules.
This way an agent may have a rule that changes the goal of the agent and thus modifies its next sequence of actions.
The general model of an agent can be seen on figure \ref{fig:agent.drawio.png}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{images/chapter1/agent.drawio.png}
    \caption{Conceptual model of an agent}\label{fig:agent.drawio.png}
\end{figure}

\subsection{Perceive}

The agent is equipped with simulated sensors that feed into its working memory.
These sensors interact with the simulated world and process the description of the world producing information that is then stored in the working memory of the agent.
Sensory information might include sight, hearing, sense of smell and other commonly thought of senses as well as abstract and arbitrary information that the agent can somehow perceive.
The combined information from all the sensors might produce information regarding the agent's awareness of the vicinity and the location of other agents in the simulated system as well as the state of the agent's body and own position in the game world.
The information is presented to the agent and it is up to the agent to decide what to do with it.
Sensors may only influence working memory of the agent.

All working memory elements produced as a result of perception of the world override the state of the working memory.
This process is called memory unification and usually results in loss of previously stored memories.
Humans in real life do not forget where they have recently been just because they walk.
To facilitate this another mechanism is needed to keep track of the old values from the overriden working memory elements.
Not every working memory element is useful to keep historic values of, however and for the sake of practicality and performance, it is up to the author to choose which values are necessary to maintain a track record of.
Depending on implementation it might make sense to create an inference rule that will save the state of some working memory elements with a prefix to be able to calculate change between the current state of the agent and its previous value.
It is up to the exact implementation to decide how and what is fed to the agent during the perception phase.

Agent's senses might be limited by factors such as weather, terrain, light level, fog, noise and similar.
This means that the information stored in the working memory is not a direct representation of the state of the game world and instead should be treated as interpretation of it done by the agent.
Rules that take into account the state of working memory should always assume that the proposed operator might fail and should use that information to influence the state of the agent's memory.
A blind agent might map out the terrain by virtue of trying to walk into walls repeatedly and keeping track of its own position (under the assumption that they are not moved externally).
This mechanism demonstrates perception feedback and shows that the agent's learning mechanisms can also be implemented by carefully crafting a set of behaviour rules.

\subsection{Think}

Agents have procedural memory that is represented by a collection of production rules that are executed in parallel.
Procedural memory is divided into inference rules and behaviour rules.
The first kind of memory elements are production rules that adjust the working memory of the agent before taking action.
The inference rules are evaluated sequentially in an undefined order and mutate the state of the working memory.
For this reason, it is essential that the designer of the agent ensures no conflicting inference rules are present.
Otherwise undefined behaviour might occurr, usually causing a rule to override the effects of another rule.
Behaviour rules do not modify the state of the working memory but instead produce operators that are used to evaluate the action the agent wishes to perform.
Each rule might take as input an arbitrary set of working memory elements (WME), check arbitrary preconditions and produce arbitrary operators that when evaluated will change the state of the simulated world by means the agent's action.
While working memory is perfect for storing the immediate description of the agent's state, it is perhaps ill suited for storage of event based information elements.
For this reason the notion of long term memory is used for storage of facts characterized by timestamp of occurrence and description of the event.
Procedural memory and production rules therein may also utilize this storage to work with temporal data, for example to facilitate the agent remembering its travel trajectory or recording events such as successful executions of certain actions.
Procedural memory is usually immutable after creation of the agent but it is possible to store a production rule inside of working memory and by means of another production rule, modify the procedural memory adding the new rule to the bank of rules the agent possesses.
This can simulate agent learning new inference rules and behaviours, for example through reading a book or being taught by other agent.

\subsection{Act}

The act of action when performed by the simulated agent is represented by an operator being evaluated.
An operator represents an action that an agent wishes to perform.
The game world is in charge of evaluating the results of the operator and rejecting it in case the agent is not allowed to perform it due to any condition that the agent itself is not aware of.
Because production rules are executed in parallel and thus output multiple operators, it is possible for conflicts to occur.
A single agent may think to move forward and back at the same time.
Because the order in which actions are performed may impact the result, it is important to define a conflict resolution strategy, either for each agent or for the whole game in general.
Such a strategy does not need to necessarily be deterministic as usually it is impossible to determine which operator should take precedence and defining conflict resolution policies for each possible set of conflicting operators is often not a viable approach.
One example of a conflict resolution strategy is a strategy that chooses a random order for conflicting operators and executes them until one is accepted.
This shows the second part of the problem which is conflict detection.
Any two operators can be conflicting in terms of resulting state of the game world.
The simplest approach is not allowing the agent to execute multiple operators at the same time and instead force it to choose one specific action instead.

\subsection{Interactions}

An agent might lack the sense of vision and instead start with its known position WME at coordinates $v_0=(0, 0)$.
It wishes to progress forward (positive $y$ axis) and so proposes the operator $Move(Agent, 0, 1)$ via a production rule with constant output.
The game world accepts the operator, evaluates it and the agent is then made aware of the operator's successful execution.
The agent now has the position WME set to $v=(0, 1)$.
It proposes the same operator again.
This time the game world rejects the action because of a wall that stands at $v=(0, 2)$.
The agent did not modify its position WME this time but added a second WME $WallAt(0, 2)$.
A production rule exists that checks if there is a $WallAt$ WME directly ahead of the current position and if so, produces operator $Move(Agent, 1, 0)$.
The production rule that produces the constant $Move(Agent, 0, 1)$ operator fires as well and afterwards the agent needs to decide which action to take.
The agent has a conflict resolution strategy defined as:

\begin{itemize}
    \item choose random
    \item fallback when action rejected
\end{itemize}

This policy means that either the lateral move will be selected by random chance or the previous operator will be selected, evaluated, subsequently rejected and a fallback would occurr to the next one.
This example shows that a virtual agent possesses a limited capacity to learn and build the information bank describing the state of the world around it.
In some implementations, the immediate position of the agent would be stored as WME while the position of each discovered wall would be put in long-term memory.

\subsection{Simulation constraints and limitations}

The simulatated environment presented in this work was designed to resemble the environment described by Grey et al.\cite{grey2011procedural} which in turn used the implementation done by Prageeth Silva\cite{silva2010shadow}.
As of the time of writing, the original implementation of Shadow Quest used in evaluation of Grey's model is unavailable and so a custom implementation with modifications was made specifically for the sake of experimentation.
Each agent may occupy a single tile in a grid-based world.
The major difference between the work doen by Grey et al. and the solution described in this thesis is the focus on information propagation.
For this reason the agents and their actions are adjusted to be compatible with the epidemiological models that can be used to model the propagation of information.
The temporal structure used in the design of the simulation is based on the concept of turns.
Because the interaction with a human player is not necessary to simulate the progression of a procedurally emergent plot, the turns are simply simulatin steps.
Within a single turn all agents execute the aforementioned decision cycle, starting from receiving information regarding the state of the world around them, deciding on the action to perform and finally executing it.
The simulation used for evaluation of the model presented in this work is limited to a set of actions that each agent may perform.

\begin{itemize}
    \item Idle - agent does nothing, stays idle for the whole turn
    \item Move - agent may decide to move in one of the four cardinal directions by a single tile
    \item Infect - agent infects another agent
\end{itemize}

Because all agents execute their actions concurrently, conflicts may occur which the simulation engine must resolve.
For the sake of simplicity, a simple conflict resolution strategy was chosen where a random agent takes priority.
The second agent is overriden to have been idle for that turn.

In terms of an infection model the agent needs to be aware of the position of agents in its field of view as well as their state of infection.
Agents use inference to choose targets to follow and propose operators such as $Infect$ or $Walk$ dependent on the working memory.
The whole process starting with percepting and ending with proposed operators is illustrated in figure \ref{fig:agent_think.drawio.png}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{images/chapter1/agent_think.drawio.png}
    \caption{Example agent's thinking process}\label{fig:agent_think.drawio.png}
\end{figure}

\subsection{Conclusions}

The chosen cognitive architecture and described solution should allow for implementation of an epidemiological model.
In order to evaluate the approach chosen in this thesis, the results of the simulation will be compared with a mathematical model directly.
The first of the research questions proposed in the thesis objective can be answered based on the analysis of the cognitive architectures described previously.
The SOAR cognitive architecture (and others similar to it) can be used for a multi-agent system to display emergent behaviour based on a set of production rules.
